{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dataset_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset_torch\n",
    "\n",
    "> Module to load the slates dataset into a Pytorch Dataset and Dataloaders with default train/valid test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import recsys_slates_dataset.data_helper as data_helper\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level='INFO')\n",
    "\n",
    "class SequentialDataset(Dataset):\n",
    "    ''' A Pytorch Dataset for the FINN Recsys Slates Dataset.\n",
    "    Attributes:\n",
    "      data: [Dict] A dictionary with tensors of the dataset. First dimension in each tensor must be the batch dimension. Requires the keys \"click\" and \"slate\". Additional elements can be added.\n",
    "      sample_candidate_items: [int] Number of negative item examples sampled from the item universe for each interaction. If positive, the dataset provide an additional dictionary item \"allitem\". Often also called uniform candidate sampling. See Eide et. al. 2021 for more information.\n",
    "    '''\n",
    "    def __init__(self, data, sample_candidate_items=0):\n",
    "\n",
    "        self.data = data\n",
    "        self.num_items = self.data['slate'].max()+1\n",
    "        self.sample_candidate_items = sample_candidate_items\n",
    "        self.mask2ind = {'train' : 1, 'valid' : 2, 'test' : 3}\n",
    "\n",
    "        logging.info(\n",
    "            \"Loading dataset with slate size={} and number of negative samples={}\"\n",
    "            .format(self.data['slate'].size(), self.sample_candidate_items))\n",
    "\n",
    "        # Performs some checks on the dataset to make sure it is valid:\n",
    "        assert \"slate\" in data.keys(), \"Slate tensor is not in dataset. This is required.\"\n",
    "        assert \"click\" in data.keys(), \"Click tensor is not in dataset. This is required.\"\n",
    "        assert all([val.size(0)==data['slate'].size(0) for key, val in data.items()]), \"Not all data tensors have the same batch dimension\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = {key: val[idx] for key, val in self.data.items()}\n",
    "\n",
    "        if self.sample_candidate_items:\n",
    "            # Sample actions uniformly (3 is the first non-special item)\n",
    "            batch['allitem'] = torch.randint(\n",
    "                size=(batch['click'].size(0), self.sample_candidate_items), \n",
    "                low=3, high=self.num_items, device = batch['click'].device\n",
    "                )\n",
    "            \n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['click'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_dataloaders(data_dir= \"dat\",\n",
    "                     batch_size=1024,\n",
    "                     num_workers= 0,\n",
    "                     sample_candidate_items=False,\n",
    "                     valid_pct= 0.05,\n",
    "                     test_pct= 0.05,\n",
    "                     t_testsplit= 5,\n",
    "                     seed=0):\n",
    "    \"\"\"\n",
    "    Loads pytorch dataloaders to be used in training. If used with standard settings, the train/val/test split is equivalent to Eide et. al. 2021.\n",
    "\n",
    "    Attributes:\n",
    "      data_dir: [str] where download and store data if not already downloaded.\n",
    "      batch_size: [int] Batch size given by dataloaders.\n",
    "      num_workers: [int] How many threads should be used to prepare batches of data.\n",
    "      sample_candidate_items: [int] Number of negative item examples sampled from the item universe for each interaction. If positive, the dataset provide an additional dictionary item \"allitem\". Often also called uniform candidate sampling. See Eide et. al. 2021 for more information.\n",
    "      valid_pct: [float] Percentage of users allocated to validation dataset.\n",
    "      test_pct: [float] Percentage of users allocated to test dataset.\n",
    "      t_testsplit: [int] For users allocated to validation and test datasets, how many initial interactions should be part of the training dataset.\n",
    "      seed: [int] Seed used to sample users/items.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Download data if not in data folder..\")\n",
    "    data_helper.download_data_files(data_dir=data_dir)\n",
    "\n",
    "    logging.info('Load data..')\n",
    "    with np.load(\"{}/data.npz\".format(data_dir)) as data_np:\n",
    "        data = {key: torch.tensor(val) for key, val in data_np.items()}\n",
    "    \n",
    "    with open('{}/ind2val.json'.format(data_dir), 'rb') as handle:\n",
    "        # Use string2int object_hook found here: https://stackoverflow.com/a/54112705\n",
    "        ind2val = json.load(\n",
    "            handle, \n",
    "            object_hook=lambda d: {\n",
    "                int(k) if k.lstrip('-').isdigit() else k: v \n",
    "                for k, v in d.items()\n",
    "                }\n",
    "            )\n",
    "\n",
    "    num_users = len(data['click'])\n",
    "    num_validusers = int(num_users * valid_pct)\n",
    "    num_testusers = int(num_users * test_pct)\n",
    "    torch.manual_seed(seed)\n",
    "    perm_user = torch.randperm(num_users)\n",
    "    valid_user_idx = perm_user[:num_validusers]\n",
    "    test_user_idx  = perm_user[num_validusers:(num_validusers+num_testusers)]\n",
    "    train_user_idx = perm_user[(num_validusers+num_testusers):]\n",
    "\n",
    "    # Split dictionary into train/valid/test with a phase mask that shows which interactions are in different sets \n",
    "    # (as some users have both train and valid data)\n",
    "    data_train = data\n",
    "    data_train['phase_mask'] = torch.ones_like(data['click']).bool()\n",
    "    data_train['phase_mask'][test_user_idx,t_testsplit:]=False\n",
    "    data_train['phase_mask'][valid_user_idx,t_testsplit:]=False\n",
    "\n",
    "    data_valid = {key: val[valid_user_idx] for key, val in data.items()}\n",
    "    data_valid['phase_mask'] = torch.zeros_like(data_valid['click']).bool()\n",
    "    data_valid['phase_mask'][:,t_testsplit:] = True\n",
    "\n",
    "    data_test = {key: val[test_user_idx] for key, val in data.items()}\n",
    "    data_test['phase_mask'] = torch.zeros_like(data_test['click']).bool()\n",
    "    data_test['phase_mask'][:,t_testsplit:] = True\n",
    "\n",
    "    data_dicts = {\n",
    "        \"train\" : data_train,\n",
    "        \"valid\" : data_valid,\n",
    "        \"test\" : data_test}\n",
    "\n",
    "    datasets = {\n",
    "        phase : SequentialDataset(data, sample_candidate_items) \n",
    "        for phase, data in data_dicts.items()\n",
    "        }\n",
    "    \n",
    "\n",
    "    # Build dataloaders for each data subset:\n",
    "    dataloaders = {\n",
    "        phase: DataLoader(ds, batch_size=batch_size, shuffle=(phase==\"train\"), num_workers=num_workers)\n",
    "        for phase, ds in datasets.items()\n",
    "    }\n",
    "    for key, dl in dataloaders.items():\n",
    "        logging.info(\n",
    "            \"In {}: num_users: {}, num_batches: {}\".format(key, len(dl.dataset), len(dl))\n",
    "        )\n",
    "    \n",
    "    # Load item attributes:\n",
    "    with np.load('{}/itemattr.npz'.format(data_dir), mmap_mode=None) as itemattr_file:\n",
    "        itemattr = {key : val for key, val in itemattr_file.items()}\n",
    "\n",
    "    return ind2val, itemattr, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 10:15:07,665 Download data if not in data folder..\n",
      "2021-08-13 10:15:07,666 Downloading data.npz\n",
      "2021-08-13 10:15:07,667 Downloading ind2val.json\n",
      "2021-08-13 10:15:07,667 Downloading itemattr.npz\n",
      "2021-08-13 10:15:07,668 Done downloading all files.\n",
      "2021-08-13 10:15:07,668 Load data..\n",
      "2021-08-13 10:15:31,565 Loading dataset with slate size=torch.Size([2277645, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-13 10:15:31,834 Loading dataset with slate size=torch.Size([2277645, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-13 10:15:31,839 Loading dataset with slate size=torch.Size([113882, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-13 10:15:31,844 Loading dataset with slate size=torch.Size([113882, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-13 10:15:31,845 In train: num_users: 2277645, num_batches: 2225\n",
      "2021-08-13 10:15:31,846 In valid: num_users: 113882, num_batches: 112\n",
      "2021-08-13 10:15:31,846 In test: num_users: 113882, num_batches: 112\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "ind2val, itemattr, dataloaders = load_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pytorch': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
